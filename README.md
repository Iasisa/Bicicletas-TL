# Pipeline ETL - Capital Bikeshare Dataset

Pipeline ETL construido con Apache Airflow para analizar patrones de uso de bicicletas compartidas en Washington D.C. (2011-2012).

## üìã Justificaci√≥n del Proyecto

Los sistemas de bicicletas compartidas representan una alternativa de movilidad sostenible que reduce la congesti√≥n vehicular, las emisiones de CO2 y promueve la salud p√∫blica en √°reas urbanas. El an√°lisis de patrones de uso de Capital Bikeshare en Washington D.C., considerando variables meteorol√≥gicas y temporales, permite optimizar la distribuci√≥n de bicicletas en estaciones, predecir demanda en horas pico y mejorar la planificaci√≥n de mantenimiento preventivo. Los beneficiarios directos incluyen autoridades de transporte urbano que pueden asignar recursos eficientemente, ciudadanos que acceden a transporte limpio y confiable, y municipios que buscan reducir la huella de carbono mediante pol√≠ticas de movilidad verde.

## üèóÔ∏è Arquitectura del Pipeline

### Estructura del Proyecto

```
PedrozoETL/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ bike_sharing_etl.py         # DAG principal (5 tareas)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hour.csv                # Dataset original (17,379 registros)
‚îÇ   ‚îî‚îÄ‚îÄ processed/                  # Outputs en formato Parquet
‚îÇ       ‚îú‚îÄ‚îÄ bike_sharing_full.parquet
‚îÇ       ‚îú‚îÄ‚îÄ daily_summary.parquet
‚îÇ       ‚îî‚îÄ‚îÄ weekly_summary.parquet
‚îú‚îÄ‚îÄ logs/                           # Logs de Airflow
‚îú‚îÄ‚îÄ plugins/                        # Custom operators (opcional)
‚îú‚îÄ‚îÄ docker-compose.yml              # Configuraci√≥n de servicios
‚îú‚îÄ‚îÄ .env                            # Variables de entorno
‚îú‚îÄ‚îÄ requirements.txt                # Dependencias Python
‚îî‚îÄ‚îÄ README.md
```

### Flujo del DAG (5 Tareas)

```
extract_and_validate
        ‚Üì
transform_clean_and_denormalize
        ‚Üì
transform_feature_engineering
        ‚Üì
transform_aggregations
        ‚Üì
load_to_parquet
```

## üîß Tareas del Pipeline

### 1Ô∏è‚É£ Extract and Validate
- Lee `hour.csv` completo (17,379 registros)
- Valida 17 columnas esperadas
- Verifica rango de fechas (2011-2012)
- Logea estad√≠sticas b√°sicas

### 2Ô∏è‚É£ Transform - Clean and Denormalize
- Elimina duplicados (si existen)
- Verifica y corrige tipos de datos
- **Desnormaliza variables clim√°ticas:**
  - `temp_celsius = temp √ó 41¬∞C`
  - `atemp_celsius = atemp √ó 50¬∞C`
  - `humidity_pct = hum √ó 100%`
  - `windspeed_kmh = windspeed √ó 67 km/h`

### 3Ô∏è‚É£ Transform - Feature Engineering
Crea nuevas features:
- `is_peak_hour`: Horas pico (7-9am, 5-7pm)
- `is_weekend`: Fin de semana (s√°bado/domingo)
- `season_name`: Primavera/Verano/Oto√±o/Invierno
- `weather_desc`: Descripci√≥n del clima
- `day_name`: Lunes-Domingo
- `month_name`: Enero-Diciembre

### 4Ô∏è‚É£ Transform - Aggregations
Crea dos niveles de agregaci√≥n:

**Agregaci√≥n Diaria:**
- Total de rentas, usuarios casuales/registrados
- Promedios de temperatura, humedad, viento
- Clima m√°s frecuente, horas pico del d√≠a

**Agregaci√≥n Semanal:**
- Totales semanales de rentas
- Promedios de m√©tricas clim√°ticas
- Rango de fechas de la semana

### 5Ô∏è‚É£ Load to Parquet
- Guarda dataset transformado completo
- Guarda agregaciones diaria y semanal
- **Formato Parquet con compresi√≥n snappy** (eficiente)

## ‚úÖ Cumplimiento de Requisitos

| Requisito | Implementaci√≥n |
|-----------|----------------|
| **Extract** | ‚úÖ Lectura de CSV con validaci√≥n completa |
| **Transform** | ‚úÖ Limpieza + Desnormalizaci√≥n + Feature Engineering + Agregaciones |
| **Load** | ‚úÖ Parquet con compresi√≥n snappy |
| **Scheduling** | ‚úÖ `@daily` a las 00:00 |
| **Error Handling** | ‚úÖ try/except en cada tarea + retries=2 |
| **Scaling** | ‚úÖ **Formato Parquet eficiente** (columnar, comprimido) |
| **Failure Notifications** | ‚úÖ Logging detallado por tarea |

---

## üöÄ Pasos de Activaci√≥n

### Prerequisitos

- Docker y Docker Compose instalados
- Al menos 4GB de RAM disponible
- Puerto 8080 libre

### Paso 1: Configurar Variables de Entorno

Edita el archivo `.env` si necesitas cambiar el UID (opcional):

```bash
# Para Linux/WSL, obt√©n tu UID:
echo $(id -u)

# Luego edita .env y actualiza AIRFLOW_UID si es diferente a 50000
```

### Paso 2: Instalar Dependencias en Airflow

Edita `.env` para agregar las dependencias:

```bash
_PIP_ADDITIONAL_REQUIREMENTS=pandas>=2.1.0 pyarrow>=14.0.0 numpy>=1.24.0
```

**O** copia el archivo `requirements.txt` al contenedor despu√©s de iniciar (ver paso 4).

### Paso 3: Levantar Airflow con Docker Compose

```bash
# Iniciar todos los servicios en background
docker-compose up -d

# Ver logs en tiempo real (opcional)
docker-compose logs -f
```

**Servicios que se levantan:**
- PostgreSQL (metadata database)
- Airflow Webserver (UI en puerto 8080)
- Airflow Scheduler (ejecutor de DAGs)

**Tiempo de inicio:** ~2-3 minutos la primera vez

### Paso 4: Instalar Dependencias Python (si no usaste .env)

```bash
# Ejecutar dentro del contenedor webserver
docker-compose exec airflow-webserver pip install pandas pyarrow numpy
```

### Paso 5: Acceder a Airflow Web UI

1. Abre tu navegador en: **http://localhost:8080**

2. **Credenciales de acceso:**
   - **Usuario:** `airflow`
   - **Contrase√±a:** `airflow`

3. Deber√≠as ver el DAG `bike_sharing_etl` en la lista

### Paso 6: Activar y Ejecutar el DAG

#### Opci√≥n A: Ejecuci√≥n Manual (Recomendado para prueba)

1. En la UI, busca el DAG **`bike_sharing_etl`**
2. Activa el toggle (switch ON) en la columna izquierda
3. Click en el nombre del DAG para ver detalles
4. Click en **"Trigger DAG"** (bot√≥n de play ‚ñ∂Ô∏è arriba a la derecha)
5. Confirma la ejecuci√≥n

#### Opci√≥n B: Esperar Ejecuci√≥n Programada

- El DAG est√° programado para ejecutarse **diariamente a las 00:00**
- Si activas el toggle, esperar√° hasta la siguiente medianoche

### Paso 7: Monitorear la Ejecuci√≥n

#### En la UI de Airflow:

1. **Graph View:** Ver el flujo de tareas y su estado
   - üü¢ Verde: Completado exitosamente
   - üîµ Azul: En ejecuci√≥n
   - üî¥ Rojo: Fall√≥
   - ‚ö™ Gris: No ejecutado a√∫n

2. **Logs de cada tarea:**
   - Click en una tarea (cuadro en el grafo)
   - Click en "Log"
   - Ver output detallado de cada funci√≥n

3. **Grid View:** Ver hist√≥rico de ejecuciones

#### Desde la terminal:

```bash
# Ver logs del scheduler
docker-compose logs -f airflow-scheduler

# Ver logs del webserver
docker-compose logs -f airflow-webserver
```

### Paso 8: Verificar Outputs Generados

```bash
# Listar archivos Parquet generados
ls -lh data/processed/

# Deber√≠as ver:
# bike_sharing_full.parquet    (~100-200 KB)
# daily_summary.parquet         (~10-20 KB)
# weekly_summary.parquet        (~5-10 KB)
```

#### Leer los archivos Parquet (Python):

```python
import pandas as pd

# Dataset completo transformado
df_full = pd.read_parquet('data/processed/bike_sharing_full.parquet')
print(f"Registros: {len(df_full):,}")
print(f"Columnas: {list(df_full.columns)}")

# Agregaci√≥n diaria
df_daily = pd.read_parquet('data/processed/daily_summary.parquet')
print(df_daily.head())

# Agregaci√≥n semanal
df_weekly = pd.read_parquet('data/processed/weekly_summary.parquet')
print(df_weekly.head())
```

---

## üõ†Ô∏è Comandos √ötiles

### Gesti√≥n de Docker Compose

```bash
# Ver estado de servicios
docker-compose ps

# Detener todos los servicios
docker-compose down

# Detener y eliminar vol√∫menes (CUIDADO: borra datos)
docker-compose down -v

# Reiniciar un servicio espec√≠fico
docker-compose restart airflow-scheduler

# Ver logs de un servicio
docker-compose logs -f airflow-webserver
```

### Gesti√≥n de Airflow

```bash
# Listar DAGs
docker-compose exec airflow-webserver airflow dags list

# Probar una tarea espec√≠fica (sin ejecutar el DAG completo)
docker-compose exec airflow-webserver airflow tasks test bike_sharing_etl extract_and_validate 2025-11-23

# Ver informaci√≥n del DAG
docker-compose exec airflow-webserver airflow dags show bike_sharing_etl

# Pausar/Despausar DAG
docker-compose exec airflow-webserver airflow dags pause bike_sharing_etl
docker-compose exec airflow-webserver airflow dags unpause bike_sharing_etl
```

### Debugging

```bash
# Ejecutar bash dentro del contenedor
docker-compose exec airflow-webserver bash

# Ver variables de entorno
docker-compose exec airflow-webserver env | grep AIRFLOW

# Ver conexiones configuradas
docker-compose exec airflow-webserver airflow connections list
```

---

## üêõ Soluci√≥n de Problemas

### Error: "Port 8080 already in use"

```bash
# Ver qu√© proceso usa el puerto 8080
sudo lsof -i :8080

# Cambiar el puerto en docker-compose.yml:
# ports:
#   - "8081:8080"  # Usa 8081 en tu m√°quina
```

### Error: "Permission denied" en logs o data

```bash
# Ajustar permisos
sudo chown -R $USER:$USER logs/ data/

# O en .env, cambiar AIRFLOW_UID a tu UID
echo "AIRFLOW_UID=$(id -u)" >> .env
```

### Error: "ModuleNotFoundError: No module named 'pandas'"

```bash
# Instalar dependencias manualmente
docker-compose exec airflow-webserver pip install pandas pyarrow numpy

# O reiniciar con .env actualizado (ver Paso 2)
docker-compose down
docker-compose up -d
```

### DAG no aparece en la UI

1. Verificar que el archivo est√° en `dags/bike_sharing_etl.py`
2. Verificar logs del scheduler:
   ```bash
   docker-compose logs airflow-scheduler | grep bike_sharing
   ```
3. Refrescar la UI (bot√≥n circular arriba a la derecha)
4. Esperar 1-2 minutos para que Airflow detecte cambios

### Tarea falla constantemente

1. Ver logs de la tarea en la UI (click en tarea ‚Üí Log)
2. Verificar que `hour.csv` existe en `data/raw/`
3. Verificar permisos de escritura en `data/processed/`

---

## üìä Dashboard

### Acceso al Dashboard

**Opci√≥n 1: Archivo Power BI**
- Ubicaci√≥n: `dashboard/Capital_Bikeshare_Dashboard.pbix`
- Instrucciones:
  1. Descargar Power BI Desktop (gratuito)
  2. Abrir el archivo .pbix
  3. Los datos est√°n embebidos, no requiere conexi√≥n adicional

**Opci√≥n 2: Screenshots**
Ver capturas en `dashboard/screenshots/`

### Visualizaciones Incluidas

1. **KPIs:**
   - Total de Rentas: 3,292,679
   - Temperatura Promedio: 20.4¬∞C

2. **Gr√°ficos:**
   - Rentas por Hora del D√≠a (columnas)
   - Distribuci√≥n por Estaci√≥n (donut)
   - Fin de Semana vs Entre Semana (barras)

### Insights Clave

- **Horas pico:** 8am y 5-6pm (horarios laborales)
- **Temporada alta:** Oto√±o (32%) y Verano (28%)
- **Uso laboral:** D√≠as entre semana tienen 2x m√°s rentas que fines de semana

## üéØ Justificaci√≥n del Proyecto

Los sistemas de bicicletas compartidas representan una alternativa de movilidad sostenible que reduce la congesti√≥n vehicular, las emisiones de CO2 y promueve la salud p√∫blica en √°reas urbanas. El an√°lisis de patrones de uso de Capital Bikeshare en Washington D.C., considerando variables meteorol√≥gicas y temporales, permite optimizar la gesti√≥n de la flota identificando horas pico de demanda y mejorar la planificaci√≥n de mantenimiento preventivo en temporadas de baja demanda. Los beneficiarios directos incluyen autoridades de transporte urbano que pueden dimensionar recursos seg√∫n patrones diarios y estacionales, ciudadanos que acceden a transporte limpio y confiable, y municipios que buscan reducir la huella de carbono mediante pol√≠ticas de movilidad verde. Este dataset facilita decisiones basadas en datos para promover sistemas de transporte m√°s sostenibles y accesibles.
---

## üìö Informaci√≥n del Dataset

- **Fuente:** Capital Bikeshare, Washington D.C.
- **Per√≠odo:** 2011-2012
- **Registros:** 17,379 (granularidad horaria)
- **Variables:** 17 columnas (clima, tiempo, conteo de rentas)
- **URL Original:** https://www.kaggle.com/datasets/lakshmi25npathi/bike-sharing-dataset

---

